---
title: "Clase 13"
author: "Arturo Maldonado"
date: "27/6/2023"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    code_download: true
    theme: cosmo
    highlight: textmate
editor_options:
  markdown:
    wrap: sentence
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Análisis de regresión como método unificado

Hasta el momento se ha usado el método de regresión lineal para analizar la relación entre dos variables numéricas, tanto la dependiente como las independientes.
Este método puede ampliarse y ser usado con una diversidad de tipos de variables independientes, tanto numéricas, como nominales y ordinales.

De esta manera, el método de regresión lineal puede entenderse como un método general, que incluye tanto el análisis de la prueba t como la prueba de ANOVA, dado que ambas usan una variable numérica como variable dependiente.

En resumen, la única condición del análisis de regresión es que la variable dependiente sea numérica, pero las variables independientes pueden ser de diverso tipo.

# Regresión lineal como prueba t de diferencia de medias

La prueba t de diferencia de medias se usaba cuando queríamos hacer una inferencia acerca de las medias entre dos grupos.
Es decir, para comparar la medida de tendencia central (la media) de una variable dependiente entre (dos) grupos de una variable independiente, que podría ser nominal u ordinal.

Siguiendo con los datos de la elección presidencial de 2016, cargamos la base de datos que hemos estado usando.

```{r base}
library(rio)
data16 = import("bases/datos2016_v3.sav")
```

Entonces, si queremos saber si existen diferencias estadísticas de la votación a Keiko Fujimori entre los departamentos de la costa y los departamentos que no están en la costa, se puede usar la prueba t de diferencia de medias.
Es decir, se quiere comparar la media de votación a Fujimori, una variable cuantitativas, entre grupos de la variable "costa", variable dicotómica.

Lo primero es ver las medias de cada grupo.

```{r medias}
mean(data16$Voto_KF_2016_1[data16$costa==0], na.rm=T)
mean(data16$Voto_KF_2016_1[data16$costa==1], na.rm=T)
```

Para analizar si la diferencia entre 19.1 y 13.9 es significativa, se puede correr una prueba t de diferencias de medias.

```{r levene}
library(DescTools)
LeveneTest(data16$Voto_PPK_2016_1, data16$costa)
```

Luego del test de igualdad de varianzas, se puede correr la prueba t.

```{r prueba t}
t.test(Voto_KF_2016_1 ~ costa, data=data16, var.equal=T)
```

De estos resultados, vemos que se obtiene un p-value de 0.08, que es menor a 0.1.
Es decir, la diferencia en el voto a PPK entre departamentos de la costa y los que no son de la costa sería significativa a 0.10.

Para graficar la relación entre ambas variables antes se ha usado un boxplot, que no muestra datos para la inferencia, solo datos descriptivos.
También se ha usado un gráfico de barras de error para mostrar las medias de cada grupo con el respectivo intervalo de confianza.

En esta sección, se produce un gráfico de dispersión entre la variable "costa" y "voto a PPK".
Debido a que la variable "costa" es dicotómica, solo tiene valores 0 y 1, por lo que los puntos, que representan a cada departamento, se agrupan en dos líneas verticales.

```{r grafico}
plot(data16$costa, data16$Voto_PPK_2016_1,
     xlab="Departamento de la costa",
     ylab="Voto a PPK (1era vuelta)",
     pch=15, ylim=c(0, 30))
abline(lm(data16$Voto_PPK_2016_1 ~ data16$costa))
```

En este gráfico también se puede pensar en una mejor recta de predicción, que reduce la distancia de los puntos a la recta.
Esta recta atraviesa por la media de cada grupo.

Por lo tanto, un modelo de regresión que use el voto a PPK en primera vuelta en 2016 como variable dependiente y la variable dicotómica "costa" como variable independiente nos daría los datos para la construcción de esta recta de aproximación.

```{r modelo1}
modelo1 = lm(data16$Voto_KF_2016_1 ~ data16$sierra)
summary(modelo1)
```

Los resultados indican que la recta es $\hat{Y} = 13.9 + 5.2*X$.
Esta recta tiene el valor de 13.9 cuando X=0.
Es decir, para los departamentos que no son de la costa, el promedio de voto a PPK es 13.9, tal cual encontramos de manera descriptiva.
Cuando X=1, el promedio de voto a PPK es 13.9+5.2 = 19.1, que es el promedio del voto para los departamentos de la costa, el mismo valor que encontramos de manera descriptiva.

Es decir la recta de aproximación pasa por el promedio del voto a PPK para los departamentos que no son de la costa y para los que sí son de la costa.

El modelo de regresión no solo replica los datos, sino también replica la prueba de significancia.
La hipótesis nula que se evalúa aquí es: $\mu_{costa} - \mu_{no costa}$.

El estadístico de la prueba t es -1.82 y un p-value de 0.08.
Estos datos también se encuentran en la línea de la variable independiente.

# Regresión lineal como ANOVA

Si quisiéremos evaluar las diferencias en el voto a Fujimori en 2016 entre las 3 regiones: costa, sierra y selva, podríamos usar el test de ANOVA.
Se tiene una variable dependiente numérica y una variable independiente nominal con 3 categorías.

Lo primero es ver las medias del voto a Fujimori para cada grupo.

```{r medias2}
mean(data16$Voto_KF_2016_1[data16$region==1], na.rm=T)
mean(data16$Voto_KF_2016_1[data16$region==2], na.rm=T)
mean(data16$Voto_KF_2016_1[data16$region==3], na.rm=T)
```

Luego, se puede evaluar si alguna de las diferencias entre estas medias es significativa mediante ANOVA.

```{r anova}
anova <- aov(data16$Voto_KF_2016_1 ~ as.factor(data16$region))
summary(anova)
TukeyHSD(anova)
```

Recordemos que ANOVA pone a prueba la siguiente hipótesis nula: $\mu_{costa} = \mu_{sierra} = \mu_{selva}$.
De acuerdo al p-value de ANOVA se concluye que debe haber al menos una diferencia significativa.
Luego, mediante la prueba de Tukey vemos que existen dos emparejamientos significativos.

Para replicar estos resultados mediante el análisis de regresión, se tiene que crear variables dummy por cada grupo de la variable "region".
Es decir, 3 variables: "costa", "sierra" y "selva", En el modelo de regresión, sin embargo, solo se debe incluir 2 de esas variables dummy (por qué?).

```{r modelo aov}
modelo2 <- lm(data16$Voto_KF_2016_1 ~  data16$costa + data16$sierra)
summary(modelo2)
```

Se excluye la tercera variable dummy debido a que los resultados para este grupo se encuentran en el intercepto en el análisis de regresión.
Es decir, cuando costa=0 y sierra=0, el voto promedio a Fujimori es 44.85, mismo valor que se encontró en el análisis descriptivo.
Cuando costa=1 y sierra=0, el voto promedio a Fujimori es 44.85-2.05 = 42.8, mismo valor que se encontró en el análisis descriptivo.
Finalmente, cuando costa=0 y sierra=1, el voto promedio a Fujimori es 44.85-13.53=31.32, mismo valor que se encontró en el análisis descriptivo.

Vemos que los coeficientes de las 2 variables dummy son las diferencias entre los grupos.
Los valores de 2.05 y 13.53 son los mismos que se evaluaron en el test de Tukey.

# Conclusión

Hemos comenzado la explicación del análisis de regresión como una forma para evaluar la relación entre dos variables numéricas.
En esta sección hemos expandido esta idea y hemos visto que el método de regresión se puede usar con variables independientes de tipo categórico, ya sea nominal u ordinal.
El único requisito es que la variable dependiente sea numérica.

De esta manera, el modelo de regresión se puede expresar

$$
\hat{Y} = \hat{\alpha} + \hat{\beta_1}*X_1+\hat{\beta_2}*X_2+\hat{\beta_3}*X_3+...+\hat{\beta_n}*X_n
$$

Los requisitos son:

-   Y es una variable numérica

-   Xs pueden ser numéricas o categóricas.

-   El número de variables independientes no puede ser mayor que el número de observaciones menos 1 (N-1).

En este curso se ha presentado el análisis de regresión lineal como técnica para evaluar la relación bivariada y para evaluar la relación multivariada.
Con la técnica de regresión multivariada se puede analizar:

-   La validez del modelo.

-   La relación de cada predictor con la variable dependiente.

-   La dirección de la relación entre cada predictor y la variable dependiente.

-   La fuerza del modelo.

-   La predicción del modelo para diferentes valores de los predictores.

# Hacia el próximo ciclo[^1]

[^1]: Si aprueban

El método de regresión lineal multivariado debe cumplir unos requisitos que no se verán en este curso.
Estos requisitos son:

-   Linealidad: que la relación entre las variables sea lineal.

-   Multicolinealidad: los predictores no deben "medir" el mismo concepto.
    Es decir, variables independientes no deben tener una alta correlación.

-   Heterocedasticidad: los errores deben ser aleatorios y no ser sistemáticos.
    Si los errores tiene una relación sistemática con los valores de la variable dependiente, se habla de heterocedasticidad.

-   Autocorrelación: en particular cuando se trabaja con una variable temporal.
    Se dice que hay autocorrelación cuando el valor en t+1 depende del valor de t.

Estos requisitos se verán en el curso de Estadística 2.

Otro tema que se verá en el siguiente curso es el modelo que se usa cuando la variable dependiente no es una numérica, sino una variable dicotómica.
Cuando tenemos esta variable dependiente se usa un modelo de regresión logística.

# Resumen

En esta sección presentamos un mapa de temas que hemos visto a lo largo de todo el semestre.
Como se ve en el esquema, las herramientas que hemos visto en el curso dependen de un punto de partida inicial, que es la distinción entre tipos de variables: cualitativas o categóricas y cuantitativas o numéricas.
Dependiendo de esta categorización, se desprende el tipo de análisis descriptivo, tanto de la tendencia central, como de la dispersión, así como las formas de graficar estas variables.

Como paso previo a la inferencia, se construyeron intervalos de confianza de la media (variables cuantitavitas) o de proporciones (variables cualitativas).
Se analizó que se podían construir intervalos de confianza de medias o proporciones para grupos.
Este es el primer paso de la inferencia.
Se evaluó si los intervalos de confianza se traslapaban o no.
Esto daba paso al análisis bivariado en la segunda parte del curso.

![](Resumen.png)

En el libro de Ritchey se muestra este árbol de decisión.
La rama derecha de "Dos muestras o dos variables" muestra el resumen de la segunda parte de este curso.
Esta segunda parte ha estado dedicada a la inferencia en el análisis bivariado.
Como hemos avanzado en este curso, se ha presentado la prueba de inferencia t, la prueba F de ANOVA, la prueba de chi-cuadrado y la regresión bivariada.
Luego, se dio el paso al análisis multivariado.

![](arbol.png)

Finalmente, el objetivo de este curso es presentar herramientas iniciales para el análisis de datos sociales.
Se espera que luego de este curso los alumnos sean consumidores o productores capaces de estadística.
Como consumidores, los alumnos van a estar expuestos a literatura especializada que use métodos estadísticos.
Las herramientas vistas en este curso permitirían que los alumnos puedan evaluar estos artículos y los hallazgos de manera crítica.

Como productores, se espera que los alumnos utilicen evidencia numérica y análisis estadístico en sus trabajos de cursos y en sus tesis de pregrado.
Ya sea para presentar análisis descriptivo o para realizar análisis inferencial, los alumnos deberían estar en la capacidad de recoger datos numéricos y trabajar con estos usando técnicas vistas en este curso.
Es completamente válido que los alumnos usen otros métodos, por ejemplo cualitativos, pero si este curso y el siguiente logran que algunos de ustedes se "atrevan" a usar métodos cuantitativos, ese será el mayor logro de este curso.
Gracias!!!
